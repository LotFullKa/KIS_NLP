{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(nn.ModuleList):\n",
    "\n",
    "\tdef __init__(self, batch_size, hidden_dim, lstm_layers, max_words):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Hyperparameters\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.LSTM_layers = lstm_layers\n",
    "\t\tself.input_size = max_words\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
    "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim*2)\n",
    "\t\tself.fc2 = nn.Linear(self.hidden_dim*2, 1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\n",
    "\t\t# Hidden and cell state definion\n",
    "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)\n",
    "\t\t\n",
    "\t\t# Initialization fo hidden and cell states\n",
    "\t\ttorch.nn.init.xavier_normal_(h)\n",
    "\t\ttorch.nn.init.xavier_normal_(c)\n",
    "\n",
    "\t\t# Each sequence \"x\" is passed through an embedding layer\n",
    "\t\tout = self.embedding(x)\n",
    "\t\t# Feed LSTMs\n",
    "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
    "\t\tout = self.dropout(out)\n",
    "\t\t# The last hidden state is taken\n",
    "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "\t\tout = self.dropout(out)\n",
    "\t\tout = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "\t\treturn out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  \n",
    "\tdef __init__(self, path, text_col_name=\"text\", label_col_name=\"target\", tokenizer=None):\n",
    "\t\t\"\"\"\n",
    "\t\tpath : path to the file\n",
    "\t\ttext_col_name, label_col_name : name or num of the column with text and label\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_csv(path)\n",
    "\t\tself.x, self.y = data[text_col_name].to_numpy(), data[label_col_name].values\n",
    "\t\t\n",
    "\t\tif tokenizer == None:\n",
    "\t\t\tself.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\t\t\n",
    "\t\tself.x = np.array(self.tokenizer(list(self.x), padding=True).input_ids)\n",
    "\t\tself.y = torch.FloatTensor(self.y)\n",
    "\n",
    "\tdef inplace(self, dataset):\n",
    "\t\tself.x = dataset[0]\n",
    "\t\tself.y = dataset[1]\n",
    "\n",
    "\tdef vocab_size(self):\n",
    "\t\treturn self.tokenizer.vocab_size\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitClassifactor(pl.LightningModule):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self.model(x)\n",
    "        loss = F.binary_cross_entropy(prediction, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self.model(x)\n",
    "        loss = F.binary_cross_entropy(prediction, y)\n",
    "        self.log(\"test_log\", loss)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=4e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "TRAIN_SIZE = 6000\n",
    "\n",
    "dataset = CustomDataset('train.csv')\n",
    "assert(TRAIN_SIZE < len(dataset))\n",
    "train_dataset, test_dataset = random_split(dataset, [6000, len(dataset) - 6000])\n",
    "train_dataset, test_dataset = train_dataset.dataset, test_dataset.dataset\n",
    "\n",
    "\n",
    "train_laoder = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory_device='cuda:0')\n",
    "test_laoder = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = TweetClassifier(BATCH_SIZE, 128, 1, train_dataset.vocab_size())\n",
    "litmodel = LitClassifactor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | TweetClassifier | 3.9 M \n",
      "------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.507    Total estimated model params size (MB)\n",
      "/home/kamil/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/kamil/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1558: PossibleUserWarning: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d841ab3ffc482a9a01972006cfb21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:592: UserWarning: pin memory device is set and pin_memory flag is not used then device pinned memory won't be usedplease set pin_memory to true, if you need to use the device pin memory\n",
      "  warnings.warn(warn_msg)\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\")\n",
    "trainer.fit(litmodel, train_laoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c92b34331a34292ba37a961ed73eab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_log            0.6833913326263428\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_log': 0.6833913326263428}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(litmodel, test_laoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
